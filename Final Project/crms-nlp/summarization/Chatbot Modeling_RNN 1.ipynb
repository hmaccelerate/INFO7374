{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Rainniee/TextSummarization_PointerGenerator/blob/master/Chatbot%20Modeling_RNN%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpbblTA9rqJp"
   },
   "source": [
    "### Load Tokenied Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anBW8fm-hwh2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Followed Adhira's binsss scripts, which generated story files and tokenized then wrote to bin\\n   I then converted the bin files to txt and made train.txt to article_abstract_tokenized.csv file\\n   The bin to txt scripts can be found in Convert BinaryFile to Text\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Followed Adhira's binsss scripts, which generated story files and tokenized then wrote to bin\n",
    "   I then converted the bin files to txt and made train.txt to article_abstract_tokenized.csv file\n",
    "   The bin to txt scripts can be found in Convert BinaryFile to Text'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195.0
    },
    "colab_type": "code",
    "id": "f-Dw_ISN1T82",
    "outputId": "66578d45-d959-47c2-fd7f-f2422b1d204c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shakespeares folios sell auction 25 m</td>\n",
       "      <td>copies william shakespeares books dubbed holy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grandmothers death saved life debt</td>\n",
       "      <td>debt 20 000 source college credit cards estima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feared life lacked meaning . cancer pushed find</td>\n",
       "      <td>late . drunk nearing 35th birthday past dank c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>texas man serving life sentence innocent doubl...</td>\n",
       "      <td>central texas man serving life sentence double...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dads reagan protests inspire stand donald trump</td>\n",
       "      <td>battling depression sleeplessness thinking fig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0             shakespeares folios sell auction 25 m    \n",
       "1                grandmothers death saved life debt    \n",
       "2   feared life lacked meaning . cancer pushed find    \n",
       "3  texas man serving life sentence innocent doubl...   \n",
       "4   dads reagan protests inspire stand donald trump    \n",
       "\n",
       "                                             article  \n",
       "0  copies william shakespeares books dubbed holy ...  \n",
       "1  debt 20 000 source college credit cards estima...  \n",
       "2  late . drunk nearing 35th birthday past dank c...  \n",
       "3  central texas man serving life sentence double...  \n",
       "4  battling depression sleeplessness thinking fig...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"article-abstract-tokenized.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "teuWkE5HmGKb"
   },
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "paqGy9em4ATs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lovelife/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Input, RepeatVector, concatenate, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops.rnn import dynamic_rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIR-iZV04hxN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "RNN_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "KEEP_PROBABILITY = 0.5\n",
    "OPTIMIZER_TYPE = 'adam'\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "data_dir_path = ''\n",
    "data_file = 'article-abstract-tokenized.csv'\n",
    "glove_file = 'glove.6B.' + str(EMBEDDING_SIZE) + 'd.txt'\n",
    "print(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGMtHroeioZQ"
   },
   "outputs": [],
   "source": [
    "VERBOSE = 1\n",
    "NUM_LAYERS = 3\n",
    "MAX_INPUT_SEQ_LENGTH = 500\n",
    "MAX_TARGET_SEQ_LENGTH = 50\n",
    "MAX_INPUT_VOCAB_SIZE = 3000\n",
    "MAX_TARGET_VOCAB_SIZE = 1000\n",
    "NUM_SAMPLES = 5000\n",
    "MAX_DECODER_SEQ_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrV2cEorizDw"
   },
   "outputs": [],
   "source": [
    "def def_keras_optimizer():\n",
    "    if OPTIMIZER_TYPE == 'sgd':\n",
    "        # default LEARNING_RATE = 0.01\n",
    "        keras_optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
    "        # default LEARNING_RATE = 0.001\n",
    "        keras_optimizer = keras.optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=None, decay=0.0)\n",
    "    elif OPTIMIZER_TYPE == 'adagrad':\n",
    "        # default LEARNING_RATE = 0.01\n",
    "        keras_optimizer = keras.optimizers.Adagrad(lr=LEARNING_RATE, epsilon=None, decay=0.0)\n",
    "    elif OPTIMIZER_TYPE == 'adadelta':\n",
    "        # default LEARNING_RATE = 1.0\n",
    "        keras_optimizer = keras.optimizers.Adadelta(lr=LEARNING_RATE, rho=0.95, epsilon=None, decay=0.0)\n",
    "    else:   # OPTIMIZER_TYPE == 'adam':\n",
    "        # default LEARNING_RATE = 0.001\n",
    "        keras_optimizer = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
    "                                                amsgrad=False)\n",
    "\n",
    "    return keras_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uR9qOZJizGG"
   },
   "outputs": [],
   "source": [
    "def def_tf_optimizer(lr):\n",
    "    if OPTIMIZER_TYPE == 'sgd':\n",
    "        # default LEARNING_RATE = 0.01\n",
    "        tf_optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
    "        # default LEARNING_RATE = 0.001\n",
    "        tf_optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "    elif OPTIMIZER_TYPE == 'adagrad':\n",
    "        # default LEARNING_RATE = 0.01\n",
    "        tf_optimizer = tf.train.AdagradOptimizer(lr)\n",
    "    elif OPTIMIZER_TYPE == 'adadelta':\n",
    "        # default LEARNING_RATE = 1.0\n",
    "        tf_optimizer = tf.train.AdadeltaOptimizer(lr)\n",
    "    else:   # OPTIMIZER_TYPE == 'adam':\n",
    "        # default LEARNING_RATE = 0.001\n",
    "        tf_optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    return tf_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVrijqnRoriA"
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzcNWMSlotH5"
   },
   "outputs": [],
   "source": [
    "def fit_text(x, y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "    if input_seq_max_length is None:\n",
    "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "    if target_seq_max_length is None:\n",
    "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "    input_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    max_input_seq_length = 0\n",
    "    max_target_seq_length = 0\n",
    "\n",
    "    for line in x:\n",
    "        text = [word for word in line.split(' ')]\n",
    "        for i, word in enumerate(text):\n",
    "            if word == '':\n",
    "                del text[i]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > input_seq_max_length:\n",
    "            text = text[0:input_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            input_counter[word] += 1\n",
    "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "    for i, line in enumerate(y):\n",
    "\n",
    "        line2 = 'START ' + str(line) + ' END'\n",
    "        text = [word for word in line2.split(' ')]\n",
    "        for j, word in enumerate(text):\n",
    "            if word == '':\n",
    "                del text[j]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > target_seq_max_length:\n",
    "            text = text[0:target_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            target_counter[word] += 1\n",
    "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "    input_word2idx = dict()\n",
    "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "        input_word2idx[word[0]] = idx + 2\n",
    "    input_word2idx['PAD'] = 0\n",
    "    input_word2idx['UNK'] = 1\n",
    "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "    target_word2idx = dict()\n",
    "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    target_word2idx['UNK'] = 0\n",
    "\n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "\n",
    "    num_input_tokens = len(input_word2idx)\n",
    "    num_target_tokens = len(target_word2idx)\n",
    "\n",
    "    config = dict()\n",
    "    config['input_word2idx'] = input_word2idx\n",
    "    config['input_idx2word'] = input_idx2word\n",
    "    config['target_word2idx'] = target_word2idx\n",
    "    config['target_idx2word'] = target_idx2word\n",
    "    config['num_input_tokens'] = num_input_tokens\n",
    "    config['num_target_tokens'] = num_target_tokens\n",
    "    config['max_input_seq_length'] = max_input_seq_length\n",
    "    config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzjKFjCio6PE"
   },
   "outputs": [],
   "source": [
    "def transform_input_text(texts, input_word2idx, max_input_seq_length):\n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        x = []\n",
    "        for word in line.lower().split(' '):\n",
    "            wid = 1\n",
    "            if word in input_word2idx:\n",
    "                wid = input_word2idx[word]\n",
    "            x.append(wid)\n",
    "            if len(x) >= max_input_seq_length:\n",
    "                break\n",
    "        temp.append(x)\n",
    "    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n",
    "\n",
    "    print(temp.shape)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2Mnxtrko-lG"
   },
   "outputs": [],
   "source": [
    "def transform_target_encoding(texts, max_target_seq_length):\n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        x = []\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        for word in line2.split(' '):\n",
    "            x.append(word)\n",
    "            if len(x) >= max_target_seq_length:\n",
    "                break\n",
    "        temp.append(x)\n",
    "\n",
    "    temp = np.array(temp)\n",
    "    print(temp.shape)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2k_-BNkpCrK"
   },
   "outputs": [],
   "source": [
    "class RecursiveRNN(object):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.num_input_tokens = config['num_input_tokens']\n",
    "        self.max_input_seq_length = config['max_input_seq_length']\n",
    "        self.num_target_tokens = config['num_target_tokens']\n",
    "        self.max_target_seq_length = config['max_target_seq_length']\n",
    "        self.input_word2idx = config['input_word2idx']\n",
    "        self.input_idx2word = config['input_idx2word']\n",
    "        self.target_word2idx = config['target_word2idx']\n",
    "        self.target_idx2word = config['target_idx2word']\n",
    "        self.config = config\n",
    "\n",
    "        inputs1 = Input(shape=(self.max_input_seq_length,))\n",
    "        am1 = Embedding(self.num_input_tokens, 128)(inputs1)\n",
    "        am2 = LSTM(128)(am1)\n",
    "\n",
    "        inputs2 = Input(shape=(self.max_target_seq_length,))\n",
    "        sm1 = Embedding(self.num_target_tokens, 128)(inputs2)\n",
    "        sm2 = LSTM(128)(sm1)\n",
    "\n",
    "        decoder1 = concatenate([am2, sm2])\n",
    "        outputs = Dense(self.num_target_tokens, activation='softmax')(decoder1)      \n",
    "\n",
    "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        optimizer = def_keras_optimizer()\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.model = model\n",
    "\n",
    "    def generate_batch(self, x_samples, y_samples, batch_size):\n",
    "        encoder_input_data_batch = []\n",
    "        decoder_input_data_batch = []\n",
    "        decoder_target_data_batch = []\n",
    "        line_idx = 0\n",
    "        while True:\n",
    "            for recordIdx in range(0, len(x_samples)):\n",
    "                target_words = y_samples[recordIdx]\n",
    "                x = x_samples[recordIdx]\n",
    "                decoder_input_line = []\n",
    "\n",
    "                for idx in range(0, len(target_words) - 1):\n",
    "                    w2idx = 0  # default [UNK]\n",
    "                    w = target_words[idx]\n",
    "                    if w in self.target_word2idx:\n",
    "                        w2idx = self.target_word2idx[w]\n",
    "                    decoder_input_line = decoder_input_line + [w2idx]\n",
    "                    decoder_target_label = np.zeros(self.num_target_tokens)\n",
    "                    w2idx_next = 0\n",
    "                    if target_words[idx + 1] in self.target_word2idx:\n",
    "                        w2idx_next = self.target_word2idx[target_words[idx + 1]]\n",
    "                    if w2idx_next != 0:\n",
    "                        decoder_target_label[w2idx_next] = 1\n",
    "                    decoder_input_data_batch.append(decoder_input_line)\n",
    "                    encoder_input_data_batch.append(x)\n",
    "                    decoder_target_data_batch.append(decoder_target_label)\n",
    "\n",
    "                    line_idx += 1\n",
    "                    if line_idx >= batch_size:\n",
    "                        yield [pad_sequences(encoder_input_data_batch, self.max_input_seq_length),\n",
    "                               pad_sequences(decoder_input_data_batch,\n",
    "                                             self.max_target_seq_length)], np.array(decoder_target_data_batch)                   \n",
    "                        line_idx = 0\n",
    "                        encoder_input_data_batch = []\n",
    "                        decoder_input_data_batch = []\n",
    "                        decoder_target_data_batch = []\n",
    "\n",
    "    def fit(self, x_train, y_train, x_test, y_test, epochs, batch_size):\n",
    "\n",
    "        y_train = transform_target_encoding(y_train, self.max_target_seq_length)\n",
    "        y_test = transform_target_encoding(y_test, self.max_target_seq_length)\n",
    "\n",
    "        x_train = transform_input_text(x_train, self.input_word2idx, self.max_input_seq_length)\n",
    "        x_test = transform_input_text(x_test, self.input_word2idx, self.max_input_seq_length)\n",
    "\n",
    "        train_gen = self.generate_batch(x_train, y_train, batch_size)\n",
    "        test_gen = self.generate_batch(x_test, y_test, batch_size)\n",
    "\n",
    "        total_training_samples = sum([len(target_text) - 1 for target_text in y_train])\n",
    "        total_testing_samples = sum([len(target_text) - 1 for target_text in y_test])\n",
    "        train_num_batches = total_training_samples // batch_size\n",
    "        test_num_batches = total_testing_samples // batch_size\n",
    "\n",
    "        self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches, epochs=epochs, verbose=VERBOSE,\n",
    "                                 validation_data=test_gen, validation_steps=test_num_batches)\n",
    "\n",
    "    def summarize(self, input_text):\n",
    "        input_seq = []\n",
    "        input_wids = []\n",
    "        for word in input_text.lower().split(' '):\n",
    "            idx = 1  # default [UNK]\n",
    "            if word in self.input_word2idx:\n",
    "                idx = self.input_word2idx[word]\n",
    "            input_wids.append(idx)\n",
    "        input_seq.append(input_wids)\n",
    "        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n",
    "        start_token = self.target_word2idx['START']\n",
    "        wid_list = [start_token]\n",
    "        sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n",
    "        terminated = False\n",
    "\n",
    "        target_text = ''\n",
    "\n",
    "        while not terminated:\n",
    "            output_tokens = self.model.predict([input_seq, sum_input_seq])\n",
    "            sample_token_idx = np.argmax(output_tokens[0, :])\n",
    "            sample_word = self.target_idx2word[sample_token_idx]\n",
    "            wid_list = wid_list + [sample_token_idx]\n",
    "\n",
    "            if sample_word != 'START' and sample_word != 'END':\n",
    "                target_text += ' ' + sample_word\n",
    "\n",
    "            if sample_word == 'END' or len(wid_list) >= self.max_target_seq_length:\n",
    "                terminated = True\n",
    "            else:\n",
    "                sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n",
    "                \n",
    "        return target_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgZXbenlpWlH"
   },
   "outputs": [],
   "source": [
    "def do_summarize(whole_content):\n",
    "    #print ('loading csv file ...')\n",
    "    df = pd.read_csv(data_dir_path + data_file)\n",
    "\n",
    "    #print('extract configuration from input texts ...')\n",
    "    y = df['abstract']\n",
    "    x = df['article']\n",
    "    config = fit_text(x, y)\n",
    "\n",
    "    #print('configuration extracted from input texts ...')\n",
    "\n",
    "    summarizer = RecursiveRNN(config)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #print('training size: ', len(x_train))\n",
    "    #print('testing size: ', len(x_test))\n",
    "\n",
    "    #print('start fitting ...')\n",
    "    #summarizer.fit(x_train, y_train, x_test, y_test, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    summarizer.model=load_model('summary_model.h5')\n",
    "    #print('start predicting ...')\n",
    "\n",
    "    ####replace new_paragraph with file content\n",
    "    abstract = summarizer.summarize(whole_content)\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump administration wants know s wrong . s . s . s . s . s . . s . s'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_content=\"I don't know what to say but sounds interesting!\"\n",
    "summary=do_summarize(whole_content)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Chatbot RNN Modeling 1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
